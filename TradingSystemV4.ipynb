{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2c/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/s2c/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonLib.helper import *\n",
    "import sklearn.preprocessing as skp\n",
    "import sqlalchemy\n",
    "# fix random seed for reproducibility\n",
    "# seed = 7\n",
    "DATA_DIR = 'data' \n",
    "# np.random.seed(seed)\n",
    "dbString = 'postgresql://s2c:JANver95@localhost:5432/stockdata'\n",
    "curInstList = 'tradeList.txt'\n",
    "engine = sqlalchemy.create_engine(dbString) \n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Activation,Dense,LSTM, Dropout,Conv1D,MaxPooling1D,Permute,Merge,Input\n",
    "from keras.layers import Flatten,BatchNormalization,LeakyReLU,GlobalAveragePooling1D,concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.models import load_model\n",
    "from pythonLib.layer_utils import AttentionLSTM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import h5py\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "tf.__version__\n",
    "\n",
    "import backtrader as bt\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import math\n",
    "import backtrader.plot as pLaut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stockList = []\n",
    "with open (curInstList) as f:\n",
    "    for each_csv in f:\n",
    "        each_csv = each_csv.rstrip('\\n') # read csv\n",
    "        curTicker = each_csv # store ticker\n",
    "        stockList.append(curTicker)\n",
    "cur = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM histdata WHERE ticker = '%s' ORDER BY datetime ASC\" % stockList[cur]\n",
    "dat = pd.read_sql(query,engine)\n",
    "utc = pytz.UTC\n",
    "startDate = utc.localize(dt.datetime(2014,3,8))\n",
    "endDate = utc.localize(dt.datetime(2017,12,30))\n",
    "backTestStart = endDate\n",
    "backTestEnd = endDate + dt.timedelta(days=7*4*10)\n",
    "res = dat[(dat['datetime'] > startDate) & (dat['datetime'] < endDate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Setup Parameters\n",
    "dataInit = res # Read the stock price data. This is 1 minute data\n",
    "data = dataInit['close'] # extract the 'close' column as a Pandas series\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.lag_plot(data) # Lag plot to check randomness\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.autocorrelation_plot(data) # Auto correlation plot to check if series is autocorrelated at all\n",
    "\n",
    "# # Find the right lag manually\n",
    "# targetCorr = 0.99 # autocorrelation we want\n",
    "# lag = findLag(data,targetCorr,True) # Lag that is indicative \n",
    "# if lag == 99: #if lag is 99 then we can just use any number above it as autocorrelation is guaranteed.\n",
    "#     lag = 120 #nice round 2  hour intervals\n",
    "# print(lag)\n",
    "lag = 60\n",
    "lookahead = 20\n",
    "flat = 1.5\n",
    "series = timeseriesLagged(data,lag + lookahead-1) # Generate the lagged series\n",
    "# res.tail(10)\n",
    "labs = range(lag+1,lag+lookahead+1)\n",
    "labs = [str(i) for i in labs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesLabs = series[labs] # labels\n",
    "seriesFeats = series.drop(labs,axis=1) #features\n",
    "seriesFeats = seriesFeats.values\n",
    "seriesLabs = seriesLabs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(seriesFeats,axis=1,keepdims=True)\n",
    "std = np.std(seriesFeats,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seriesFeatsSubbed = seriesFeats - mean\n",
    "seriesFeatsNormed = seriesFeatsSubbed/std \n",
    "seriesLabsSubbed = seriesLabs - mean\n",
    "seriesLabsNormed = seriesLabsSubbed/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x,y,Mean,Std = shuffle(seriesFeatsNormed,seriesLabsNormed,mean,std)\n",
    "x,y,Mean,Std = seriesFeatsNormed,seriesLabsNormed,mean,std\n",
    "x = x.reshape(-1,lag,1)\n",
    "y = y.reshape(-1,lookahead)\n",
    "tot = len(x)\n",
    "y = y\n",
    "trainPercent = 0.7 # majority of data used for training\n",
    "testPercent = 0.95 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:,:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "trainMean = Mean[0:int(trainPercent*tot),:]\n",
    "trainStd = Std[0:int(trainPercent*tot),:]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:,:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "testMean = Mean[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "testStd = Std[int(trainPercent*tot): int(testPercent*tot),:]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:]\n",
    "yVal = y[int(testPercent*tot):]\n",
    "valMean = Mean[int(testPercent*tot):]\n",
    "valStd = Std[int(testPercent*tot):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert xTrain.shape[0] == yTrain.shape[0]\n",
    "assert xTest.shape[0] == yTest.shape[0]\n",
    "assert xVal.shape[0] == yVal.shape[0]\n",
    "\n",
    "learnRate = 0.05\n",
    "batchSize = 100\n",
    "totalBatches = (xTrain.shape[0]//batchSize)\n",
    "epochs = 5\n",
    "\n",
    "nClasses = 2\n",
    "nLength = xTrain.shape[1]\n",
    "inputShape = (nLength,1)\n",
    "# xTrainDataSet = tf.data.Dataset.from_tensors(xTrain)\n",
    "# xTrainIter = xTrainDataSet.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs = Input(shape=inputShape)\n",
    "\n",
    "# LSTM1 = LSTM(45,\n",
    "#         activation=\"tanh\",name=\"LSTM1\")(inputs)\n",
    "# FCN1 = Dense(20,activation='relu',name=\"FCN1\")(LSTM1)\n",
    "# FCN2 = Dense(10,activation='relu',name=\"FCN2\")(FCN1)\n",
    "# Preds = Dense(1,activation='linear',name=\"Linear\")(FCN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "#https://arxiv.org/pdf/1709.05206.pdf LSTM-FCN\n",
    "def buyConvModel():\n",
    "    buyModelConv = Sequential()\n",
    "    buyModelConv.add(Conv1D(112, \n",
    "                            kernel_size= 2, \n",
    "                            strides=1, \n",
    "                            input_shape=inputShape,\n",
    "                            batch_size = None,\n",
    "#                            kernel_regularizer=regularizers.l1(0.01)\n",
    "                           )              \n",
    "                     )\n",
    "    buyModelConv.add(BatchNormalization())\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "#     buyModelConv.add(Dropout(0.8))\n",
    "    \n",
    "    \n",
    "    buyModelConv.add(Conv1D(256, kernel_size= 4, strides=1,\n",
    "#                             kernel_regularizer=regularizers.l1(0.01)\n",
    "                           ))\n",
    "    buyModelConv.add(BatchNormalization())\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    buyModelConv.add(Conv1D(384, kernel_size= 4, strides=1))\n",
    "     buyModelConv.add(Activation('relu'))\n",
    "    buyModelConv.add(Conv1D(384, kernel_size= 4, strides=1))\n",
    "     buyModelConv.add(Activation('relu'))\n",
    "    buyModelConv.add(Conv1D(256, kernel_size= 4, strides=1))\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "\n",
    "    im = buyModelConv.layers[0].input\n",
    "    buyConvInput = buyModelConv(im)\n",
    "\n",
    "    return [buyConvInput,im]\n",
    "l = buyConvModel()\n",
    "output = Dense(20, activation='relu')(l[0])\n",
    "# output1 = Dense(20, activation='relu')(output)\n",
    "# output2 = Dense(20, activation='linear')(output1)\n",
    "buyModel = Model(inputs=l[1],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_157_input (InputLayer (None, 60, 1)             0         \n",
      "_________________________________________________________________\n",
      "sequential_48 (Sequential)   (None, 47, 256)           1494032   \n",
      "=================================================================\n",
      "Total params: 1,494,032\n",
      "Trainable params: 1,493,296\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = Model(inputs=inputs, outputs=Preds)\n",
    "buyModel.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=Adam(lr=learnRate),\n",
    "              metrics=['mse']\n",
    "             )\n",
    "\n",
    "buyModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-2.26931294, -1.32113205, -1.41595014, ..., -3.21749384,\n        -3.21749384, -3.21749384],\n       [-1.22973126, -1.32082246, -2.1406433 , ..., -3.05155535,\n        -3.05155535, -3.05155535],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-b762c0e17c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxVal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m              epochs = 5 )\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-2.26931294, -1.32113205, -1.41595014, ..., -3.21749384,\n        -3.21749384, -3.21749384],\n       [-1.22973126, -1.32082246, -2.1406433 , ..., -3.05155535,\n        -3.05155535, -3.05155535],..."
     ]
    }
   ],
   "source": [
    "buyModel.fit(x=xTrain,\n",
    "             y=yTrain, \n",
    "#              class_weight=classWeight,\n",
    "             validation_data = (xVal,yVal),\n",
    "             batch_size = batchSize,\n",
    "             epochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPreds = model.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([503.6 , 503.6 , 503.55, ..., 535.95, 537.7 , 538.  ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTested = (yTest*testStd) +testMean\n",
    "yTested[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([503.29414521, 503.61998976, 503.52789347, ..., 539.63304938,\n",
       "       539.58929365, 539.62036137])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPredFin = (yPreds * testStd) + testMean\n",
    "yPredFin[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xFin = xTest.reshape(-1,lag) * testStd + testMean\n",
    "xFin = xFin[:,-1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "resAct = yTested[:,-1].reshape((-1,1))-xFin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resPred = yPredFin[:,-1].reshape(-1,1) - xFin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resActBinned = np.where(resAct<=0,-1,1)\n",
    "resPredBinned = np.where(resPred<=0,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       ...,\n",
       "       [-1],\n",
       "       [-1],\n",
       "       [-1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resActBinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32620,  2848],\n",
       "       [30466,  3453]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(resActBinned,resPredBinned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AlgoTrading]",
   "language": "python",
   "name": "conda-env-AlgoTrading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
