{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import talib.abstract as ta\n",
    "import tensorflow\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    convertfunc = lambda x: (pd.to_datetime(x,utc=True)).tz_convert('Asia/Kolkata')\n",
    "    return pd.read_csv(filename,\n",
    "                    names=[\"DateTime\",\"open\",\"high\",\"low\",\"close\",\"volume\"],\n",
    "                    dtype=None,\n",
    "                    delimiter = ',',\n",
    "                    converters = {0:convertfunc},\n",
    "                  #  index_col = 0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A2Z = readData(\"data/A2Z.csv\")\n",
    "Nifty50 = readData(\"data/Nifty50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making sure that 2 timeseries are synced to the smaller time series \n",
    "def sanitizeTimeSeries(ts1,ts2):\n",
    "    # If TS1 is not bigger, then make TS1 the bigger one and TS2 the smaller one.\n",
    "    flipped = 0\n",
    "    if len(ts2) > len(ts1):\n",
    "        flipped = 1\n",
    "        ts1,ts2 = ts2,ts1\n",
    "    for dt in ts1[\"DateTime\"].values:\n",
    "        if dt in ts2['DateTime'].values:\n",
    "            continue\n",
    "        else:\n",
    "            #print(dt)\n",
    "            ts1.drop(ts1[ts1[\"DateTime\"]==dt].index,inplace = True)\n",
    "    if flipped:\n",
    "        return ts2, ts1.reset_index(drop = True)\n",
    "    else:\n",
    "        return ts1.reset_index(drop = True), ts2      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the time series that will be used for prediction\n",
    "Nifty50Data,A2ZData = sanitizeTimeSeries(Nifty50,A2Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=60):\n",
    "    df = DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # need to split into examples of length T such that T-n are used as predictors to predict the Tth element\n",
    "# def generateTSeries(ts,T):\n",
    "#     predictors = []\n",
    "#     predictee = []\n",
    "#     # Drop if there aren't enough rows to make a set of T\n",
    "#     if len(ts)%T != 0:\n",
    "#         ts = ts.drop(ts.tail(len(ts)%T).index,inplace = True)\n",
    "#     # Start grouping\n",
    "#     predictee = ts.iloc[T-1::T,:]\n",
    "#     tsNew = ts.drop(ts.iloc[T-1::T,:].index)\n",
    "#     for count,i in enumerate(range(0,len(ts),T-1)):\n",
    "#         if tsNew.iloc[i:i+T-1].empty:\n",
    "#             continue\n",
    "#         predictors.append((tsNew.iloc[i:i+T-1]))\n",
    "#     return predictors,predictee\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictors,predictee = generateTSeries(A2ZData,60)\n",
    "# predictors2, _ = generateTSeries(Nifty50Data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictorsFin = []\n",
    "# for predictor in predictors:\n",
    "#     predictorsFin.append(predictor['close'].values)\n",
    "# predictorsFin = np.array(predictorsFin)\n",
    "# predicteeFin = np.array(predictee['close'].values)\n",
    "# # Code for later:\n",
    "# # yTemp = np.zeros(x.shape[0])\n",
    "# # cur = predicteeFin[0]\n",
    "# # for count,i in enumerate(predicteeFin):\n",
    "# # #     print(i-cur < -0.30)\n",
    "# #     if count==0:\n",
    "# #         continue\n",
    "# #     if i - cur > 0.30:\n",
    "# #         yTemp[count] = 1 #Buy\n",
    "# #         cur = i\n",
    "# #     elif i - cur < -0.30:\n",
    "# #         yTemp[count] = -1 #Sell\n",
    "# #         cur = i\n",
    "# #     else:\n",
    "# #         yTemp[count] = 0 #Do Nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# s = np.arange(predictorsFin.shape[0])\n",
    "# np.random.shuffle(s)\n",
    "# x = predictorsFin[s]\n",
    "# y = predicteeFin[s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 59, 1)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #transform the data into train and test\n",
    "# from sklearn import preprocessing\n",
    "# # scaler_x=preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "# # x=np.array(x).reshape(x.shape[0],x.shape[1])\n",
    "# # x=scaler_x.fit_transform(x)\n",
    "\n",
    "# # scaler_y=preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "# # y=np.array(y).reshape((len(y),1))\n",
    "# # y=scaler_y.fit_transform(y)\n",
    "\n",
    "# #the train set\n",
    "# train_end= int(0.8*x.shape[0])\n",
    "# x_train=x[0:train_end,]\n",
    "# x_test=x[train_end:]\n",
    "# y_train=y[0:train_end]\n",
    "# y_test=y[train_end:]\n",
    "# x_train=x_train.reshape(x_train.shape+(1,))\n",
    "# x_test=x_test.reshape(x_test.shape+(1,))\n",
    "# x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30621"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train.shape\n",
    "# y_train.shape\n",
    "# 519*59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AlgoTrading]",
   "language": "python",
   "name": "conda-env-AlgoTrading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
