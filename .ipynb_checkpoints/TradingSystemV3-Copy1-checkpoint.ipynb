{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2c/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/s2c/anaconda3/envs/AlgoTrading/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import time\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonLib.helper import *\n",
    "import sklearn.preprocessing as skp\n",
    "import sqlalchemy\n",
    "# fix random seed for reproducibility\n",
    "# seed = 7\n",
    "DATA_DIR = 'data' \n",
    "# np.random.seed(seed)\n",
    "dbString = 'postgresql://s2c:JANver95@localhost:5432/stockdata'\n",
    "curInstList = 'tradeList.txt'\n",
    "engine = sqlalchemy.create_engine(dbString) \n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Activation,Dense,LSTM, Dropout,Conv1D,MaxPooling1D,Permute,Merge,Input\n",
    "from keras.layers import Flatten,BatchNormalization,LeakyReLU,GlobalAveragePooling1D,concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers import Reshape\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.models import load_model\n",
    "from pythonLib.layer_utils import AttentionLSTM\n",
    "from pythonLib.modwt import modwt,modwtmra\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import h5py\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "tf.__version__\n",
    "\n",
    "import backtrader as bt\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import math\n",
    "import backtrader.plot as pLaut\n",
    "import pywt\n",
    "# dataInit = readData('data/SBIN.csv') \n",
    "import talib as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into postgres\n",
    "\n",
    "We need to load the data into a postgres database. First, we go through each file appending the file name as an added column, then we store each file into the database under the HistoricalData table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loads everything into postgres, Uncomment if not needed\n",
    "# stockList = []\n",
    "# # dataInit = dataInit\n",
    "# with open (curInstList) as f:\n",
    "#     for each_csv in f:\n",
    "#         each_csv = each_csv.rstrip('\\n') # read csv\n",
    "#         curTicker = each_csv # store ticker\n",
    "#         stockList.append(curTicker)\n",
    "#         File = os.path.join(DATA_DIR,each_csv) # join the file path\n",
    "#         File = File + \".csv\" # add csv at the end\n",
    "#         print(File) # diagnostic print\n",
    "        \n",
    "#         try:\n",
    "#             dataInit = readData(File) # read the actual file\n",
    "# #             dataInit.replace([\"NaN\", 'NaT','NULL'], np.nan, inplace = True)\n",
    "# #             print(dataInit.shape)\n",
    "# #             dataInit.dropna(axis=0,how='any',inplace=True)\n",
    "# #             print(dataInit.shape)\n",
    "#         except:\n",
    "#             print(each_csv)\n",
    "            \n",
    "#         deleteOld = \"DELETE FROM histdata WHERE ticker = '%s'\" % curTicker # delete old references\n",
    "#         connection = engine.connect() # delete old references\n",
    "#         result = connection.execute(deleteOld) # delete old references\n",
    "#         connection.close() # delete old references\n",
    "# #         print(\"Deleted old references\")/\n",
    "        \n",
    "#         height = np.shape(dataInit)[0]\n",
    "#         width = 1\n",
    "#         tickers = pd.DataFrame(each_csv, index=range(height), columns=range(width))\n",
    "#         tickers.columns = ['ticker']\n",
    "#         dataInit = tickers.join(dataInit) # black magic v1\n",
    "#         dataInit['datetime'] = dataInit['datetime'].apply(lambda d: str(d)) # Black Magic v2\n",
    "#         dataInit.dropna().to_sql(\"histdata\",engine,index = False,dtype={'datetime':sqlalchemy.TIMESTAMP(timezone=True)},if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZEEL', 'CIPLA', 'RELIANCE', 'TATASTEEL', 'SUNPHARMA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockList = []\n",
    "with open (curInstList) as f:\n",
    "    for each_csv in f:\n",
    "        each_csv = each_csv.rstrip('\\n') # read csv\n",
    "        curTicker = each_csv # store ticker\n",
    "        stockList.append(curTicker)\n",
    "cur = 0\n",
    "# stockList[0] = 'RCOM'\n",
    "# stockList[0] = 'BANKBARODA'\n",
    "stockList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database, retrieve a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZEEL', 'CIPLA', 'RELIANCE', 'TATASTEEL', 'SUNPHARMA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"SELECT ticker,avg(close),avg(volume) FROM histdata GROUP BY ticker ORDER BY avg(volume) DESC\"\n",
    "# stockSet = pd.read_sql(query,engine)\n",
    "# pd.options.display.max_rows = 4000\n",
    "# stockSet\n",
    "# stockList.append('SBIN')\n",
    "stockList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "query = \"SELECT * FROM histdata WHERE ticker = '%s' ORDER BY datetime ASC\" % stockList[cur]\n",
    "dat = pd.read_sql(query,engine)\n",
    "dat.datetime= dat.datetime.dt.tz_convert('Asia/Calcutta')\n",
    "# utc = pytz.UTC\n",
    "startDate = dt.datetime(2014,3,8)\n",
    "endDate = dt.datetime(2017,12,31)\n",
    "backTestStart = endDate\n",
    "backTestEnd = endDate + dt.timedelta(days=7*4*10)\n",
    "res = dat[(dat['datetime'] > startDate) & (dat['datetime'] < endDate)]\n",
    "# res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vol = res['volume']\n",
    "openP = res['open']\n",
    "high = res['high']\n",
    "low = res['low']\n",
    "data = res['close']\n",
    "# res.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "close5SMA = ta.SMA(data,5).dropna().reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n",
    "\n",
    "These functions are more or less general functions that should prove to be fairly useful\n",
    "\n",
    "\n",
    "- **ReadData(filename)** : Reads data from Zerodha API historical data files and returns a Pandas DataFrame\n",
    "- **sycTimeSeries(ts1,ts2)** : Making sure that 2 timeseries are synced to the smaller time series\n",
    "- **timeseriesLagged(data, lag=60)**: Creates Lagged series.Goes through a series and generates an lag+1  dimensional   pandas DataFrame that has each previous lag timeunit.\n",
    "- **binarizeTime(resLagged, rate=0.01)** : Binarizes the last column into 1,-1 or 0 depending whether the price increased, decreased or stayed the same from the beginning to the end of the lag period (triggers on changes by magnitutde = rate*current price).\n",
    "- **findLag(data, targetCorr,suppressed)** :  Finds the right lag given a target correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading some Data and Getting a feel \n",
    "\n",
    "We use an autocorrelation plot to help us figure out what is an optimal amount of lag. We are really looking for a lag that correlates highly. We go through the lags till we reach the last lag that guarantees 0.97 autocorrelation\n",
    "\n",
    "## THIS DID NOT WORK AS EXPECTED. REPLACE WITH FALSE NEAREST NEIGHBOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Setup Parameters\n",
    "dataInit = res # Read the stock price data. This is 1 minute data\n",
    "# data = dataInit['close'] # extract the 'close' column as a Pandas series\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.lag_plot(data) # Lag plot to check randomness\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.autocorrelation_plot(data) # Auto correlation plot to check if series is autocorrelated at all\n",
    "\n",
    "# # Find the right lag manually\n",
    "# targetCorr = 0.99 # autocorrelation we want\n",
    "# lag = findLag(data,targetCorr,True) # Lag that is indicative \n",
    "# if lag == 99: #if lag is 99 then we can just use any number above it as autocorrelation is guaranteed.\n",
    "#     lag = 120 #nice round 2  hour intervals\n",
    "# print(lag)\n",
    "lag = 45\n",
    "lookahead = 15\n",
    "flat = 1.5\n",
    "series = timeseriesLagged(data,lag + lookahead-1) # Generate the lagged series\n",
    "vols = timeseriesLagged(vol,lag + lookahead-1)\n",
    "lows = timeseriesLagged(low,lag + lookahead-1)\n",
    "opens = timeseriesLagged(openP,lag+lookahead-1)\n",
    "highs = timeseriesLagged(high,lag+lookahead-1)\n",
    "# res.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate the series for volumes. We need to drop the last column at some point as it is irrelevant.\n",
    "volsSeries = binarizeTime(vols,0,lookahead = lookahead, flat= flat)\n",
    "volsSeries = volsSeries.drop(str(lag+1),axis=1)\n",
    "#standardize\n",
    "# \n",
    "# volsSeries = pywt.dwt(volsSeries,'haar',3)[0] #+ pywt.dwt(volsSeries,'haar',3)[1]\n",
    "volsSeries = skp.minmax_scale(volsSeries,axis=1)\n",
    "# wtVols = np.transpose(wt,(1,2,0))\n",
    "# np.shape(pywt.dwt(volsSeries,'fbsp1')[1])\n",
    "# volsSeries.shape\n",
    "# wavelet = modwt('db2') \n",
    "# volsTransformed=pywt.dwt(volsSeries,wavelet)\n",
    "# np.shape(volsTransformed)\n",
    "# plt.plot(wtVols[0])\n",
    "plt.plot(volsSeries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the series for volumes. We need to drop the last column at some point as it is irrelevant.\n",
    "lowsSeries = binarizeTime(lows,0,lookahead = lookahead, flat= flat)\n",
    "lowsSeries = lowsSeries.drop(str(lag+1),axis=1)\n",
    "#standardize\n",
    "# \n",
    "# volsSeries = pywt.dwt(volsSeries,'haar',3)[0] #+ pywt.dwt(volsSeries,'haar',3)[1]\n",
    "lowsSeries = skp.minmax_scale(lowsSeries,axis=1)\n",
    "# wtVols = np.transpose(wt,(1,2,0))\n",
    "# np.shape(pywt.dwt(volsSeries,'fbsp1')[1])\n",
    "# volsSeries.shape\n",
    "# wavelet = modwt('db2') \n",
    "# volsTransformed=pywt.dwt(volsSeries,wavelet)\n",
    "# np.shape(volsTransformed)\n",
    "# plt.plot(wtVols[0])\n",
    "plt.plot(lowsSeries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate the series for volumes. We need to drop the last column at some point as it is irrelevant.\n",
    "highsSeries = binarizeTime(highs,0,lookahead = lookahead, flat= flat)\n",
    "highsSeries = highsSeries.drop(str(lag+1),axis=1)\n",
    "#standardize\n",
    "# \n",
    "# volsSeries = pywt.dwt(volsSeries,'haar',3)[0] #+ pywt.dwt(volsSeries,'haar',3)[1]\n",
    "highsSeries = skp.minmax_scale(highsSeries,axis=1)\n",
    "# wtVols = np.transpose(wt,(1,2,0))\n",
    "# np.shape(pywt.dwt(volsSeries,'fbsp1')[1])\n",
    "# volsSeries.shape\n",
    "# wavelet = modwt('db2') \n",
    "# volsTransformed=pywt.dwt(volsSeries,wavelet)\n",
    "# np.shape(volsTransformed)\n",
    "# plt.plot(wtVols[0])\n",
    "plt.plot(highsSeries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the series for volumes. We need to drop the last column at some point as it is irrelevant.\n",
    "opensSeries = binarizeTime(opens,0,lookahead = lookahead, flat= flat)\n",
    "opensSeries = opensSeries.drop(str(lag+1),axis=1)\n",
    "#standardize\n",
    "# \n",
    "# volsSeries = pywt.dwt(volsSeries,'haar',3)[0] #+ pywt.dwt(volsSeries,'haar',3)[1]\n",
    "opensSeries = skp.minmax_scale(opensSeries,axis=1)\n",
    "# wtVols = np.transpose(wt,(1,2,0))\n",
    "# np.shape(pywt.dwt(volsSeries,'fbsp1')[1])\n",
    "# volsSeries.shape\n",
    "# wavelet = modwt('db2') \n",
    "# volsTransformed=pywt.dwt(volsSeries,wavelet)\n",
    "# np.shape(volsTransformed)\n",
    "# plt.plot(wtVols[0])\n",
    "plt.plot(opensSeries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# closeSMA = binarizeTime(series,0,lookahead = lookahead, flat= flat)\n",
    "# closeSMA = closeSMA.drop(str(lag+1),axis=1)\n",
    "# closeSMA = pd.rolling_mean(data,window=lag)\n",
    "# closeSMA = closeSMA[lag-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary series where 0 = hold and 1 = buy\n",
    "buySeries = binarizeTime(series,0,lookahead = lookahead, flat= flat,atleast=0.2)\n",
    "change = buySeries.iloc[:,-1]== -1 # convert to binary\n",
    "buySeries.loc[change,str(lag+1)]=0 # convert to binary\n",
    "                                   # clean up post binary\n",
    "\n",
    "buySeriesLabs = buySeries[str(lag+1)] # labels\n",
    "buySeriesFeats = buySeries.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "# stanardize\n",
    "# buySeriesFeats = skp.minmax_scale(buySeriesFeats,axis=1)\n",
    "# func = lambda a:modwt(a, 'haar',3)\n",
    "# buySeriesFeats = pywt.dwt(buySeriesFeats,'haar',3)[0]# + pywt.dwt(buySeriesFeats,'haar',3)[1]\n",
    "buySeriesFeats = skp.minmax_scale(buySeriesFeats,axis=1)\n",
    "# wt = np.transpose(wt,(1,2,0))\n",
    "# Convert the data into a suitable format\n",
    "\n",
    "buySeries = np.zeros((len(volsSeries),buySeriesFeats.shape[-1],5))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "# buySeries[:,:,1] = opensSeries\n",
    "# buySeries[:,:,2] = highsSeries\n",
    "# buySeries[:,:,3] = lowsSeries\n",
    "buySeries[:,:,4] = volsSeries\n",
    "# # buySeries[0,:,1]\n",
    "# type(buySeriesFeats)\n",
    "buySeriesFeats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create binary series where 0 = hold and 1 = sell\n",
    "sellSeries = binarizeTime(series,0,lookahead=lookahead,flat=flat,atleast=0.2)\n",
    "change = sellSeries.iloc[:,-1]== 1 # find 1s and convert to 0\n",
    "sellSeries.loc[change,str(lag+1)]=0 # \n",
    "change = sellSeries.iloc[:,-1]== -1 # find -1 and conver to 1s\n",
    "sellSeries.loc[change,str(lag+1)]= 1 # convert to\n",
    "                                     # cleanup post binary\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Convert the data into a suitable format\n",
    "sellSeriesLabs = sellSeries[str(lag+1)]\n",
    "sellSeriesFeats = sellSeries.drop(str(lag+1),axis=1)\n",
    "# Convert the data into a suitable format\n",
    "# sellSeriesFeats = pywt.dwt(sellSeriesFeats,'haar',3)[0]# + pywt.dwt(sellSeriesFeats,'haar',3)[1]\n",
    "sellSeriesFeats = skp.minmax_scale(sellSeriesFeats,axis=1)\n",
    "# wt = np.transpose(wt,(1,2,0))\n",
    "# Convert the data into a suitable format\n",
    "\n",
    "sellSeries = np.zeros((len(volsSeries),sellSeriesFeats.shape[-1],5))\n",
    "sellSeries[:,:,0] = sellSeriesFeats\n",
    "sellSeries[:,:,1] = opensSeries\n",
    "sellSeries[:,:,2] = highsSeries\n",
    "sellSeries[:,:,3] = lowsSeries\n",
    "sellSeries[:,:,4] = volsSeries\n",
    "# # buySeries[0,:,1]\n",
    "# type(buySeriesFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellSeries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data\n",
    "\n",
    "Now that we have an idea of what's going on in the dataset, it is a good time to generate training data. We do an 90:20 training:testing split, and then we randomize the training set because we assume that only the last LAG minutes matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x,y = shuffle(buySeries,buySeriesLabs)\n",
    "x,y = buySeries,buySeriesLabs\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "trainPercent = 0.9 # majority of data used for training\n",
    "testPercent = 0.9 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:,:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:,:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:,:]\n",
    "yVal = y[int(testPercent*tot):]\n",
    "\n",
    "# #Reshape for keras\n",
    "# xTrain = xTrain.reshape(xTrain.shape[0], xTrain.shape[1],1)\n",
    "# xTest = xTest.reshape(xTest.shape[0], xTest.shape[1],1)\n",
    "# xVal = xVal.reshape(xVal.shape[0],xVal.shape[1],1)\n",
    "\n",
    "\n",
    "# # # encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(yTrain)\n",
    "# encodedyTrain = encoder.transform(yTrain)\n",
    "# encodedyTest = encoder.transform(yTest)\n",
    "# encodedyVal = encoder.transform(yVal)\n",
    "# # convert integers to one hot encoded\n",
    "# yTrain = np_utils.to_categorical(encodedyTrain)\n",
    "# yTest = np_utils.to_categorical(encodedyTest)\n",
    "# yVal = np_utils.to_categorical(encodedyVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Compute Class weights\n",
    "classWeight = class_weight.compute_class_weight('balanced', np.unique(yTrain), yTrain)\n",
    "classWeight = dict(enumerate(classWeight))\n",
    "# classWeight[1]=classWeight[1]*5\n",
    "print(classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert xTrain.shape[0] == yTrain.shape[0]\n",
    "assert xTest.shape[0] == yTest.shape[0]\n",
    "assert xVal.shape[0] == yVal.shape[0]\n",
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet for Buy\n",
    "\n",
    "A CNN to predict buy signals from the above generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learnRate = 0.0001\n",
    "batchSize = 128\n",
    "totalBatches = (xTrain.shape[0]//batchSize)\n",
    "epochs = 5\n",
    "\n",
    "nClasses = 2\n",
    "nLength = xTrain.shape[1]\n",
    "inputShape = (nLength,1)\n",
    "# xTrainDataSet = tf.data.Dataset.from_tensors(xTrain)\n",
    "# xTrainIter = xTrainDataSet.make_one_shot_iterator()\n",
    "sum(y==1)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "#https://arxiv.org/pdf/1709.05206.pdf LSTM-FCN\n",
    "def buyConvModel():\n",
    "    buyModelConv = Sequential()\n",
    "    buyModelConv.add(Conv1D(128, kernel_size= 8, strides=1, \n",
    "                      input_shape=inputShape,\n",
    "                     batch_size = None))\n",
    "    buyModelConv.add(BatchNormalization())\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "#     buyModelConv.add(Dropout(0.8))\n",
    "    \n",
    "    \n",
    "    buyModelConv.add(Conv1D(256, kernel_size= 5, strides=1))\n",
    "    buyModelConv.add(BatchNormalization())\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "#     buyModelConv.add(Dropout(0.8))\n",
    "\n",
    "#     buyModelConv.add(Conv1D(512, kernel_size= 5, strides=1))\n",
    "#     buyModelConv.add(BatchNormalization())\n",
    "#     buyModelConv.add(Activation('relu'))\n",
    "# #     buyModelConv.add(Dropout(0.8))\n",
    "    \n",
    "#     buyModelConv.add(Conv1D(256,kernel_size= 5, strides=1))\n",
    "#     buyModelConv.add(BatchNormalization())\n",
    "#     buyModelConv.add(Activation('relu'))\n",
    "# #     buyModelConv.add(Dropout(0.8))\n",
    "\n",
    "    buyModelConv.add(Conv1D(128,kernel_size= 3, strides=1))\n",
    "    buyModelConv.add(BatchNormalization())\n",
    "    buyModelConv.add(Activation('relu'))\n",
    "\n",
    "#     buyModelConv.add(GlobalAveragePooling1D())\n",
    "    buyModelConv.add(Reshape((-1,)))\n",
    "    # convInput = Input(shape=(None,8))\n",
    "    im = buyModelConv.layers[0].input\n",
    "    buyConvInput = buyModelConv(im)\n",
    "\n",
    " ########################################\n",
    "    buyModelLSTM = Sequential()\n",
    "    buyModelLSTM.add(Permute((2, 1), input_shape=inputShape))\n",
    "    buyModelLSTM.add(LSTM(32))\n",
    "    buyModelLSTM.add(Dropout(0.8))\n",
    "#     buyModelLSTM.add(LSTM(128))\n",
    "#     buyModelLSTM.add(Dropout(0.4))\n",
    "    im2 = buyModelLSTM.layers[0].input\n",
    "    buyLstmInput = buyModelLSTM(im2)\n",
    "#     merged = concatenate([buyConvInput, buyLstmInput])\n",
    "    \n",
    "    return [[buyConvInput,buyLstmInput],[im,im2]]\n",
    "#############################\n",
    "l = np.array([buyConvModel() for i in range(1)])\n",
    "merged = concatenate(list(l[:,0,:].flatten()))\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "buyModel = Model(inputs=list(l[:,1,:].flatten()),outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buyModel.summary()\n",
    "buyModel.compile(loss=binary_crossentropy,\n",
    "              optimizer=Adam(lr=learnRate),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = xTrain[:,:,0].reshape(-1,lag,1)\n",
    "# x2 = xTrain[:,:,1].reshape(-1,lag,1)\n",
    "# x3 = xTrain[:,:,2].reshape(-1,lag,1)\n",
    "# x4 = xTrain[:,:,3].reshape(-1,lag,1)\n",
    "x5 = xTrain[:,:,4].reshape(-1,lag,1)\n",
    "\n",
    "x1Val = xVal[:,:,0].reshape(-1,lag,1)\n",
    "# x2Val = xVal[:,:,1].reshape(-1,lag,1)\n",
    "# x3Val = xVal[:,:,2].reshape(-1,lag,1)\n",
    "# x4Val = xVal[:,:,3].reshape(-1,lag,1)\n",
    "x5Val = xVal[:,:,4].reshape(-1,lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyModel.fit(x=[x1,x1],\n",
    "             y=yTrain, \n",
    "             class_weight=classWeight,\n",
    "             validation_data = ([x1Val,x1Val],yVal),\n",
    "             batch_size = batchSize,\n",
    "             epochs = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x1 = xVal[:,:,0].reshape(-1,lag,1)\n",
    "# x2 = xVal[:,:,1].reshape(-1,lag,1)\n",
    "# x3 = xVal[:,:,2].reshape(-1,lag,1)\n",
    "# x4 = xVal[:,:,3].reshape(-1,lag,1)\n",
    "# x5 = xVal[:,:,4].reshape(-1,lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = buyModel.evaluate([x1Val,x1Val], yVal, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "yProbs = np.array(buyModel.predict([x1Val,x1Val])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPred = yProbs > 0.5\n",
    "tn, fp, fn, tp = confusion_matrix(yVal, yPred).ravel()\n",
    "print(tp)\n",
    "(tp)/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "tot = 0\n",
    "for curr,gt in enumerate(yVal):\n",
    "#     print(gt)\n",
    "    if gt == 1 and yPred[curr] > 0.5:\n",
    "        TP += 1\n",
    "    elif gt != 1 and yPred[curr] > 0.5:\n",
    "        FP += 1\n",
    "    elif gt == 0 and yPred[curr] < 0.5:\n",
    "        TN += 1\n",
    "    elif gt != -1 and yPred[curr] < 0.5:\n",
    "        FN += 1\n",
    "print(sum(y==1))\n",
    "print(TP)\n",
    "TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(yVal==1)/len(yVal))\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ConvNet for Sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x,y = shuffle(sellSeries,sellSeriesLabs)\n",
    "x,y = sellSeries,sellSeriesLabs\n",
    "tot = len(x)\n",
    "y = y.values\n",
    "trainPercent = 0.6 # majority of data used for training\n",
    "testPercent = 0.6 # \n",
    "valPercent = 1.00  #\n",
    "\n",
    "# Test Train Val Split\n",
    "\n",
    "xTrain = x[0:int(trainPercent*tot),:,:]\n",
    "yTrain = y[0:int(trainPercent*tot)]\n",
    "\n",
    "xTest = x[int(trainPercent*tot): int(testPercent*tot),:,:]\n",
    "yTest = y[int(trainPercent*tot): int(testPercent*tot)]\n",
    "\n",
    "xVal = x[int(testPercent*tot):,:,:]\n",
    "yVal = y[int(testPercent*tot):]\n",
    "\n",
    "# #Reshape for keras\n",
    "# xTrain = xTrain.reshape(xTrain.shape[0], xTrain.shape[1],1)\n",
    "# xTest = xTest.reshape(xTest.shape[0], xTest.shape[1],1)\n",
    "# xVal = xVal.reshape(xVal.shape[0],xVal.shape[1],1)\n",
    "\n",
    "\n",
    "# # # encode class values as integers\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(yTrain)\n",
    "# encodedyTrain = encoder.transform(yTrain)\n",
    "# encodedyTest = encoder.transform(yTest)\n",
    "# encodedyVal = encoder.transform(yVal)\n",
    "# # convert integers to one hot encoded\n",
    "# yTrain = np_utils.to_categorical(encodedyTrain)\n",
    "# yTest = np_utils.to_categorical(encodedyTest)\n",
    "# yVal = np_utils.to_categorical(encodedyVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute Class weights\n",
    "classWeight = class_weight.compute_class_weight('balanced', np.unique(yTrain), yTrain)\n",
    "classWeight = dict(enumerate(classWeight))\n",
    "# classWeight[-1] = classWeight.pop(0)\n",
    "xTest.shape\n",
    "assert xTrain.shape[0] == yTrain.shape[0]\n",
    "assert xTest.shape[0] == yTest.shape[0]\n",
    "assert xVal.shape[0] == yVal.shape[0]\n",
    "yTrain\n",
    "learnRate = 0.001\n",
    "batchSize = 64\n",
    "totalBatches = (xTrain.shape[0]//batchSize)\n",
    "epochs = 5\n",
    "\n",
    "nClasses = 2\n",
    "nLength = xTrain.shape[1]\n",
    "inputShape = (nLength,1)\n",
    "# xTrainDataSet = tf.data.Dataset.from_tensors(xTrain)\n",
    "# xTrainIter = xTrainDataSet.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(yTrain==1)/len(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "#https://arxiv.org/pdf/1709.05206.pdf LSTM-FCN\n",
    "def sellConvModel():\n",
    "    sellModelConv = Sequential()\n",
    "    sellModelConv.add(Conv1D(128, kernel_size= 8, strides=1, \n",
    "                      input_shape=inputShape,\n",
    "                     batch_size = None))\n",
    "    sellModelConv.add(BatchNormalization())\n",
    "    sellModelConv.add(Activation('relu'))\n",
    "#     sellModelConv.add(MaxPooling1D())\n",
    "    \n",
    "    sellModelConv.add(Conv1D(256,kernel_size= 5, strides=1))\n",
    "    sellModelConv.add(BatchNormalization())\n",
    "    sellModelConv.add(Activation('relu'))\n",
    "    sellModelConv.add(MaxPooling1D())\n",
    "\n",
    "    sellModelConv.add(Conv1D(128, kernel_size= 3, strides=1))\n",
    "    sellModelConv.add(BatchNormalization())\n",
    "    sellModelConv.add(Activation('relu'))\n",
    "#     sellModelConv.add(Dropout(0.5))\n",
    "#     sellModelConv.add(MaxPooling1D())\n",
    "    \n",
    "#     sellModelConv.add(Conv1D(90,kernel_size= 2, strides=1))\n",
    "#     sellModelConv.add(BatchNormalization())\n",
    "#     sellModelConv.add(Activation('relu'))\n",
    "#     sellModelConv.add(MaxPooling1D())\n",
    "\n",
    "#     sellModelConv.add(Conv1D(45, kernel_size= 2, strides=1))\n",
    "#     sellModelConv.add(BatchNormalization())\n",
    "#     sellModelConv.add(Activation('relu'))\n",
    "#     sellModelConv.add(MaxPooling1D())\n",
    "    # sellModelConv.add(Conv1D(15,kernel_size= 2, strides=1))\n",
    "    # sellModelConv.add(BatchNormalization())\n",
    "    # sellModelConv.add(Activation('relu'))\n",
    "\n",
    "#     sellModelConv.add(GlobalAveragePooling1D())\n",
    "    # convInput = Input(shape=(None,8))\n",
    "    sellModelConv.add(GlobalAveragePooling1D())\n",
    "    im = sellModelConv.layers[0].input\n",
    "    sellConvInput = sellModelConv(im)\n",
    "#     return sellConvInput,im\n",
    " ########################################\n",
    "    sellModelLSTM = Sequential()\n",
    "    sellModelLSTM.add(Permute((2,1), input_shape=inputShape))\n",
    "    sellModelLSTM.add(LSTM(64))\n",
    "    sellModelLSTM.add(Dropout(0.8))\n",
    "    im2 = sellModelLSTM.layers[0].input\n",
    "    sellLstmInput = sellModelLSTM(im2)\n",
    "    \n",
    "    return [[sellConvInput,sellLstmInput],[im,im2]]\n",
    "#############################\n",
    "\n",
    "l = np.array([sellConvModel() for i in range(2)])\n",
    "merged = concatenate(list(l[:,0,:].flatten()))\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "sellModel = Model(inputs=list(l[:,1,:].flatten()),outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sellModel.summary()\n",
    "sellModel.compile(loss=binary_crossentropy,\n",
    "              optimizer=Adam(lr=learnRate),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = xTrain[:,:,0].reshape(-1,lag,1)\n",
    "# x2 = xTrain[:,:,1].reshape(-1,lag,1)\n",
    "# x3 = xTrain[:,:,2].reshape(-1,lag,1)\n",
    "# x4 = xTrain[:,:,3].reshape(-1,lag,1)\n",
    "x5 = xTrain[:,:,4].reshape(-1,lag,1)\n",
    "# type(x1)\n",
    "x1Val = xVal[:,:,0].reshape(-1,lag,1)\n",
    "# x2 = xTrain[:,:,1].reshape(-1,lag,1)\n",
    "# x3 = xTrain[:,:,2].reshape(-1,lag,1)\n",
    "# x4 = xTrain[:,:,3].reshape(-1,lag,1)\n",
    "x5Val = xVal[:,:,4].reshape(-1,lag,1)\n",
    "# type(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sellModel.fit(x=[x1,x1,x5,x5],\n",
    "             y=yTrain, \n",
    "             class_weight=classWeight,\n",
    "             validation_data = ([x1Val,x1Val,x5Val,x5Val],yVal),\n",
    "             batch_size = batchSize,\n",
    "             epochs = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = xVal[:,:,0].reshape(-1,lag,1)\n",
    "# x2 = xVal[:,:,1].reshape(-1,lag,1)\n",
    "# x3 = xVal[:,:,2].reshape(-1,lag,1)\n",
    "# x4 = xVal[:,:,3].reshape(-1,lag,1)\n",
    "x5 = xVal[:,:,4].reshape(-1,lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = sellModel.evaluate([x1,x1,x5,x5], yTest, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "yProbs = np.array(sellModel.predict([x1,x1,x5,x5])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "query = \"SELECT * FROM histdata WHERE ticker = '%s' ORDER BY datetime ASC\" % stockList[cur]\n",
    "dat = pd.read_sql(query,engine)\n",
    "dat.datetime= dat.datetime.dt.tz_convert('Asia/Calcutta')\n",
    "# utc = pytz.UTC\n",
    "startDate = dt.datetime(2018,1,1)\n",
    "endDate = dt.datetime(2018,3,15)\n",
    "backTestStart = endDate\n",
    "backTestEnd = endDate + dt.timedelta(days=7*4*10)\n",
    "res = dat[(dat['datetime'] > startDate) & (dat['datetime'] < endDate)]\n",
    "# res\n",
    "vol = res['volume']\n",
    "openP = res['open']\n",
    "high = res['high']\n",
    "low = res['low']\n",
    "# res.tail(100)\n",
    "\n",
    "# # Setup Parameters\n",
    "dataInit = res # Read the stock price data. This is 1 minute data\n",
    "data = dataInit['close'] # extract the 'close' column as a Pandas series\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.lag_plot(data) # Lag plot to check randomness\n",
    "# plt.figure()\n",
    "# pd.tools.plotting.autocorrelation_plot(data) # Auto correlation plot to check if series is autocorrelated at all\n",
    "\n",
    "# # Find the right lag manually\n",
    "# targetCorr = 0.99 # autocorrelation we want\n",
    "# lag = findLag(data,targetCorr,True) # Lag that is indicative \n",
    "# if lag == 99: #if lag is 99 then we can just use any number above it as autocorrelation is guaranteed.\n",
    "#     lag = 120 #nice round 2  hour intervals\n",
    "# print(lag)\n",
    "series = timeseriesLagged(data,lag + lookahead-1) # Generate the lagged series\n",
    "vols = timeseriesLagged(vol,lag + lookahead-1)\n",
    "lows = timeseriesLagged(low,lag + lookahead-1)\n",
    "opens = timeseriesLagged(openP,lag+lookahead-1)\n",
    "highs = timeseriesLagged(high,lag+lookahead-1)\n",
    "# res.tail(10)\n",
    "\n",
    "# generate the series for volumes. We need to drop the last column at some point as it is irrelevant.\n",
    "volsSeries = binarizeTime(vols,0,lookahead = lookahead, flat= flat)\n",
    "volsSeries = volsSeries.drop(str(lag+1),axis=1)\n",
    "\n",
    "volsSeries = skp.minmax_scale(volsSeries,axis=1)\n",
    "\n",
    "plt.plot(volsSeries[0])\n",
    "\n",
    "# Create binary series where 0 = hold and 1 = buy\n",
    "buySeries = binarizeTime(series,0,lookahead = lookahead, flat= flat,atleast=0.4)\n",
    "change = buySeries.iloc[:,-1]== -1 # convert to binary\n",
    "buySeries.loc[change,str(lag+1)]=0 # convert to binary\n",
    "                                   # clean up post binary\n",
    "buySeriesLabs = buySeries[str(lag+1)] # labels\n",
    "buySeriesFeats = buySeries.drop(str(lag+1),axis=1) #features\n",
    "buySeriesFeats = buySeriesFeats.values\n",
    "buySeriesFeats = skp.minmax_scale(buySeriesFeats,axis=1)\n",
    "buySeries = np.zeros((len(volsSeries),buySeriesFeats.shape[-1],5))\n",
    "buySeries[:,:,0] = buySeriesFeats\n",
    "# buySeries[:,:,1] = opensSeries\n",
    "# buySeries[:,:,2] = highsSeries\n",
    "# buySeries[:,:,3] = lowsSeries\n",
    "buySeries[:,:,4] = volsSeries\n",
    "\n",
    "# Create binary series where 0 = hold and 1 = sell\n",
    "sellSeries = binarizeTime(series,0,lookahead=lookahead,flat=flat,atleast=0.4)\n",
    "change = sellSeries.iloc[:,-1]== 1 # find 1s and convert to 0\n",
    "sellSeries.loc[change,str(lag+1)]=0 # \n",
    "change = sellSeries.iloc[:,-1]== -1 # find -1 and conver to 1s\n",
    "sellSeries.loc[change,str(lag+1)]= 1 # convert to\n",
    "                                     # cleanup post binary \n",
    "# Convert the data into a suitable format\n",
    "sellSeriesLabs = sellSeries[str(lag+1)]\n",
    "sellSeriesFeats = sellSeries.drop(str(lag+1),axis=1)\n",
    "sellSeriesFeats = skp.minmax_scale(sellSeriesFeats,axis=1)\n",
    "sellSeries = np.zeros((len(volsSeries),sellSeriesFeats.shape[-1],5))\n",
    "sellSeries[:,:,0] = sellSeriesFeats\n",
    "# sellSeries[:,:,1] = opensSeries\n",
    "# sellSeries[:,:,2] = highsSeries\n",
    "# sellSeries[:,:,3] = lowsSeries\n",
    "sellSeries[:,:,4] = volsSeries\n",
    "# # buySeries[0,:,1]\n",
    "# type(buySeriesFeats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX = binarizeTime(series,0,lookahead=lookahead,flat=flat,atleast=0.4)\n",
    "sellSeriesLabs = testX[str(lag+1)]\n",
    "sellSeriesFeats = testX.drop(str(lag+1),axis=1)\n",
    "sellSeriesFeats = skp.minmax_scale(sellSeriesFeats,axis=1)\n",
    "# wt = np.transpose(wt,(1,2,0))\n",
    "# Convert the data into a suitable format\n",
    "\n",
    "sellSeries = np.zeros((len(volsSeries),sellSeriesFeats.shape[-1],5))\n",
    "sellSeries[:,:,0] = sellSeriesFeats\n",
    "# sellSeries[:,:,1] = opensSeries\n",
    "# sellSeries[:,:,2] = highsSeries\n",
    "# sellSeries[:,:,3] = lowsSeries\n",
    "sellSeries[:,:,4] = volsSeries\n",
    "\n",
    "x,y = shuffle(sellSeries,sellSeriesLabs)\n",
    "\n",
    "x1 = x[:,:,0].reshape(-1,lag,1)\n",
    "x5 = x[:,:,4].reshape(-1,lag,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPredSell = np.array(sellModel.predict([x1,x1,x5,x5])).flatten()\n",
    "yPredBuy = np.array(buyModel.predict([x1,x1,x5,x5])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "tot = 0\n",
    "for curr,gt in enumerate(y):\n",
    "#     print(gt)\n",
    "    if gt == 1 and yPredSell[curr] < 0.45 and yPredBuy[curr] > 0.65:\n",
    "        TP += 1\n",
    "    elif gt != 1 and yPredSell[curr] < 0.45 and yPredBuy[curr] > 0.65:\n",
    "        FP += 1\n",
    "    elif gt == -1 and yPredSell[curr] > 0.65 and yPredBuy[curr] < 0.45:\n",
    "        TP += 1\n",
    "    elif gt != -1 and yPredSell[curr] > 0.65 and yPredBuy[curr] < 0.45:\n",
    "        FP += 1\n",
    "print(len(y))\n",
    "print(TP)\n",
    "TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(y))\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buyModel.save('modelsFin/%sbuyModel.h5' % stockList[cur])\n",
    "sellModel.save('modelsFin/%ssellModel.h5' % stockList[cur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement backtester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# endDate = utc.localize(dt.datetime(2017,3,31))\n",
    "# # endDate+dt.timedelta(days=1)\n",
    "# # endDate\n",
    "# finDat = dat[(dat['datetime'] > endDate+dt.timedelta(days=1)) & (dat['datetime'] < endDate+dt.timedelta(days=7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Plotter(pLaut.Plot):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # custom color for volume up bars \n",
    "\n",
    "    def show(self):\n",
    "        mng = plt\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        title = str(backTestStart.date()) + \" to \" + str(backTestEnd.date())\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"plots/\" + title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printTradeAnalysis(analyzer):\n",
    "    #Get the results we are interested in\n",
    "    try:\n",
    "        total_open = analyzer.total.open\n",
    "        total_closed = analyzer.total.closed\n",
    "        total_won = analyzer.won.total\n",
    "        total_lost = analyzer.lost.total\n",
    "        win_streak = analyzer.streak.won.longest\n",
    "        lose_streak = analyzer.streak.lost.longest\n",
    "        pnl_net = round(analyzer.pnl.net.total,2)\n",
    "        strike_rate = round((total_won / total_closed) * 100, 2)\n",
    "        #Designate the rows\n",
    "        h1 = ['Total Open', 'Total Closed', 'Total Won', 'Total Lost']\n",
    "        h2 = ['Strike Rate','Win Streak', 'Losing Streak', 'PnL Net']\n",
    "        r1 = [total_open, total_closed,total_won,total_lost]\n",
    "        r2 = [strike_rate, win_streak, lose_streak, pnl_net]\n",
    "        #Check which set of headers is the longest.\n",
    "        if len(h1) > len(h2):\n",
    "            header_length = len(h1)\n",
    "        else:\n",
    "            header_length = len(h2)\n",
    "        #Print the rows\n",
    "        print_list = [h1,r1,h2,r2]\n",
    "        row_format =\"{:<15}\" * (header_length + 1)\n",
    "        print(\"Trade Analysis Results:\")\n",
    "        for row in print_list:\n",
    "            print(row_format.format('',*row))\n",
    "    except:\n",
    "        print(\"No trades!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class neuralModel(bt.Indicator):\n",
    "    lines = ('Ind',)\n",
    "    params = (('period', 30),('neuralModel',None))\n",
    "\n",
    "    def __init__(self):\n",
    "        self.addminperiod(self.params.period)\n",
    "#         self.i = 0\n",
    "\n",
    "    def next(self):\n",
    "        vols = np.array(self.data.volume.get(size=self.p.period)) # get the volumes\n",
    "        close = np.array(self.data.close.get(size=self.p.period)) # get the closing prices\n",
    "        low = np.array(self.data.low.get(size=self.p.period))\n",
    "        open = np.array(self.data.open.get(size=self.p.period))\n",
    "        high = np.array(self.data.high.get(size=self.p.period))\n",
    "        # scale them\n",
    "#         vols = pywt.dwt(vols,'haar',3)[0] + pywt.dwt(vols,'haar',3)[1]\n",
    "        vols = skp.minmax_scale(vols)\n",
    "        # scale them\n",
    "#         close = pywt.dwt(close,'haar',3)[0] + pywt.dwt(close,'haar',3)[1]       \n",
    "        close = skp.minmax_scale(close)\n",
    "#         print(wt)\n",
    "        # Convert the data into a suitable format\n",
    "        low = skp.minmax_scale(low)\n",
    "        open = skp.minmax_scale(open)\n",
    "        high = skp.minmax_scale(high)\n",
    "\n",
    "        data = np.zeros((1,close.shape[-1],5))\n",
    "        data[0,:,0] = close\n",
    "        data[0,:,1] = open\n",
    "        data[0,:,2] = high\n",
    "        data[0,:,3] = low\n",
    "        data[0,:,4] = vols\n",
    "        x1 = data[:,:,0].reshape(-1,lag,1)\n",
    "#         x2 = data[:,:,1].reshape(-1,lag,1)\n",
    "#         x3 = data[:,:,2].reshape(-1,lag,1)\n",
    "#         x4 = data[:,:,3].reshape(-1,lag,1)\n",
    "        x5 = data[:,:,4].reshape(-1,lag,1)\n",
    "#         print(wt)\n",
    "        prob = self.p.neuralModel.predict([x1,x1,x5,x5])[0][0]\n",
    "#         print(prob)\n",
    "        self.lines.Ind[0] = prob # predict and round to 0 for no action and 1 for buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lag = 45\n",
    "# cur = 0\n",
    "print('modelsFin/%sbuyModel.h5' % stockList[cur])\n",
    "\n",
    "# buyModel = load_model('modelsFin/%sbuyModel.h5' % stockList[cur], custom_objects={'AttentionLSTM': AttentionLSTM}) # load the buy model\n",
    "# sellModel = load_model('modelsFin/%ssellModel.h5' % stockList[cur], custom_objects={'AttentionLSTM': AttentionLSTM}) # load the sell model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestStrategy(bt.Strategy):\n",
    "    params = (\n",
    "        ('lagPeriod', lag),\n",
    "        ('buyNeural',buyModel),\n",
    "        ('SellNeural',sellModel)\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.dataclose = self.datas[0].close\n",
    "\n",
    "        self.neuralBuy = neuralModel(\n",
    "            self.datas[0], \n",
    "            period=self.params.lagPeriod, \n",
    "            neuralModel = self.params.buyNeural,\n",
    "            plot = False\n",
    "        )\n",
    "\n",
    "        self.neuralSell = neuralModel(\n",
    "            self.datas[0], \n",
    "            period=self.params.lagPeriod, \n",
    "            neuralModel = self.params.SellNeural,\n",
    "            plot = False\n",
    "        )\n",
    "\n",
    "\n",
    "    def next(self):\n",
    "#         print(self.neuralBuy[0])\n",
    "#         print(self.neuralSell[0])\n",
    "\n",
    "\n",
    "\n",
    "        if self.neuralBuy[0] > 0.65 and self.neuralSell[0] < 0.45:\n",
    "#             print(self.neuralBuy[0])\n",
    "#             print(self.neuralSell[0])\n",
    "    \n",
    "            buyOrd = self.buy_bracket(limitprice=self.dataclose+1.5,\n",
    "                                      price=self.dataclose,\n",
    "                                      stopprice=self -2,\n",
    "                                      size = 1000,\n",
    "                                      valid = 0\n",
    "                                     )\n",
    "\n",
    "\n",
    "        elif self.neuralSell[0] > 0.65 and self.neuralBuy[0] < 0.45:\n",
    "#             print(self.neuralBuy[0])\n",
    "#             print(self.neuralSell[0])\n",
    "\n",
    "            sellOrd = self.sell_bracket(limitprice=self.dataclose-1.5,\n",
    "                          price=self.dataclose,\n",
    "                          stopprice=self.dataclose + 2,\n",
    "                          size = 1000,\n",
    "                          valid = 0)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install backtrader==1.9.60.122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cur = 1\n",
    "print('data/%s.csv' % stockList[cur])\n",
    "query = \"SELECT * FROM histdata WHERE ticker = '%s' AND datetime BETWEEN '2018-01-01' AND '2018-03-17'ORDER BY datetime ASC\" % stockList[cur]\n",
    "datBacktest = pd.read_sql(query,engine)\n",
    "datBacktest.datetime= datBacktest.datetime.dt.tz_convert('Asia/Calcutta')\n",
    "\n",
    "datBacktest\n",
    "stockList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fed = bt.feeds.PandasData(dataname=datBacktest.dropna(),\n",
    "#                               dtformat=\"%Y-%m-%dT%H:%M:%S%z\",\n",
    "                                openinterest=-1,   \n",
    "                                datetime=1,\n",
    "                                open=2,\n",
    "                                high = 3,\n",
    "                                low = 4,\n",
    "                                close = 5,\n",
    "                                volume = 6,\n",
    "#                               headers=False,\n",
    "#                               fromdate= backTestStart,\n",
    "#                               todate= backTestEnd,\n",
    "#                               timeframe=bt.TimeFrame.Minutes,\n",
    "#                               tzinput = pytz.timezone('Asia/Kolkata'),\n",
    "                              plot=True)\n",
    "\n",
    "\n",
    "brokerageCom = ((0.0001 +0.0000325)*0.18) + (0.0001 +0.0000325) + 0.00025 + 0.00002\n",
    "print(brokerageCom)\n",
    "cerebro = bt.Cerebro()\n",
    "cerebro.broker.set_shortcash(False)\n",
    "cerebro.broker.setcash(100000000)\n",
    "cerebro.broker.setcommission(commission=brokerageCom  ,margin = False)\n",
    "cerebro.adddata(fed) \n",
    "cerebro.addstrategy(TestStrategy,plot=False)\n",
    "cerebro.addobserver(bt.observers.Value)\n",
    "cerebro.addobserver(bt.observers.Trades)\n",
    "cerebro.addobserver(bt.observers.BuySell)\n",
    "# cerebro.addanalyzer(bt.analyzers.SharpeRatio , _name='Sharpe',timeframe = bt.TimeFrame.Minutes)\n",
    "cerebro.addanalyzer(bt.analyzers.Returns , _name='Transactions', timeframe = bt.TimeFrame.Minutes)\n",
    "cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name=\"ta\")\n",
    "print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "# Tracer()()\n",
    "thestrats = cerebro.run(stdstats=False)\n",
    "\n",
    "thestrat = thestrats[0]\n",
    "\n",
    "print('returns:', thestrat.analyzers.Transactions.get_analysis())\n",
    "printTradeAnalysis(thestrat.analyzers.ta.get_analysis())\n",
    "print(thestrat.analyzers.ta.get_analysis())\n",
    "\n",
    "print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "cerebro.plot(start=backTestStart , end=backTestEnd,plotter = Plotter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(thestrat.analyzers.ta.get_analysis()['short'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(thestrat.analyzers.ta.get_analysis()['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "tot = 0\n",
    "for curr,gt in enumerate(y):\n",
    "#     print(gt)\n",
    "    if gt == 1 and yPredSell[curr] < 0.45 and yPredBuy[curr] > 0.75:\n",
    "        TP += 1\n",
    "    elif gt != 1 and yPredSell[curr] < 0.45 and yPredBuy[curr] > 0.75:\n",
    "        FP += 1\n",
    "    elif gt == -1 and yPredSell[curr] > 0.75 and yPredBuy[curr] < 0.45:\n",
    "        TP += 1\n",
    "    elif gt != -1 and yPredSell[curr] > 0.75 and yPredBuy[curr] < 0.45:\n",
    "        FP += 1\n",
    "print(len(y))\n",
    "print(TP)\n",
    "TP/(TP+FP)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AlgoTrading]",
   "language": "python",
   "name": "conda-env-AlgoTrading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
