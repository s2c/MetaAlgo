{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonLib.helper import *\n",
    "import sqlalchemy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "DATA_DIR = 'data' \n",
    "np.random.seed(seed)\n",
    "dbString = 'postgresql://s2c:JANver95@localhost:5432/stockdata'\n",
    "engine = sqlalchemy.create_engine(dbString) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into postgres\n",
    "\n",
    "We need to load the data into a postgres database. First, we go through each file appending the file name as an added column, then we store each file into the database under the HistoricalData table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loads everything into postgres, Uncomment if needed\n",
    "# # i = 0\n",
    "# for each_csv in os.listdir(DATA_DIR):\n",
    "#     i = i+1\n",
    "#     File = os.path.join(DATA_DIR,each_csv)\n",
    "#     try:\n",
    "#         dataInit = readData(File)\n",
    "#     except:\n",
    "#         print(each_csv)\n",
    "#     height = np.shape(dataInit)[0]\n",
    "#     width = 1\n",
    "#     tickers = pd.DataFrame(each_csv[:-4], index=range(height), columns=range(width))\n",
    "#     tickers.columns = ['ticker']\n",
    "#     dataInit = tickers.join(dataInit)\n",
    "#     dataInit['datetime'] = dataInit['datetime'].apply(lambda d: str(d))\n",
    "#     engine = sqlalchemy.create_engine('postgresql://s2c:JANver95@localhost:5432/stockdata')\n",
    "#     dataInit.to_sql(\"histdata\",engine,index = False,dtype={'datetime':sqlalchemy.TIMESTAMP(timezone=True)},if_exists='append')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database, retrieve a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = \"SELECT ticker,avg(volume) FROM histdata GROUP BY ticker ORDER BY avg DESC\"\n",
    "print(query)\n",
    "stockSet = pd.read_sql(query,engine)\n",
    "\n",
    "stockSet\n",
    "\n",
    "# We use this to select DLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM histdata WHERE ticker = 'DLF'\"\n",
    "res = pd.read_sql(query,engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n",
    "\n",
    "These functions are more or less general functions that should prove to be fairly useful\n",
    "\n",
    "\n",
    "- **ReadData(filename)** : Reads data from Zerodha API historical data files and returns a Pandas DataFrame\n",
    "- **sycTimeSeries(ts1,ts2)** : Making sure that 2 timeseries are synced to the smaller time series\n",
    "- **timeseriesLagged(data, lag=60)**: Creates Lagged series.Goes through a series and generates an lag+1  dimensional   pandas DataFrame that has each previous lag timeunit.\n",
    "- **binarizeTime(resLagged, rate=0.01)** : Binarizes the last column into 1,-1 or 0 depending whether the price increased, decreased or stayed the same from the beginning to the end of the lag period (triggers on changes by magnitutde = rate*current price).\n",
    "- **findLag(data, targetCorr,suppressed)** :  Finds the right lag given a target correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading some Data and Getting a feel \n",
    "\n",
    "We use an autocorrelation plot to help us figure out what is an optimal amount of lag. We are really looking for a lag that correlates highly. We go through the lags till we reach the last lag that guarantees 0.97 autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup Parameters\n",
    "dataInit = res # Read the stock price data. This is 1 minute data\n",
    "data = dataInit['close'] # extract the 'close' column as a Pandas series\n",
    "plt.figure()\n",
    "pd.tools.plotting.lag_plot(data) # Lag plot to check randomness\n",
    "plt.figure()\n",
    "pd.tools.plotting.autocorrelation_plot(data) # Auto correlation plot to check if series is autocorrelated at all\n",
    "\n",
    "# Find the right lag manually\n",
    "targetCorr = 0.99 # autocorrelation we want\n",
    "lag = findLag(data,targetCorr,True) # Lag that is indicative \n",
    "if lag == 99: #if lag is 99 then we can just use any number above it as autocorrelation is guaranteed.\n",
    "    lag = 120 #nice round 2  hour intervals\n",
    "print(lag)\n",
    "series = timeseriesLagged(data,lag) # Generate the lagged series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binSeries = binarizeTime(series,0.0)\n",
    "binSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data\n",
    "\n",
    "Now that we have an idea of what's going on in the dataset, it is a good time to generate training data. We do an 80:20 training:testing split, and then we randomize the training set because we assume that only the last LAG minutes matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get values from pandas series as we need a numpy array for our classifier\n",
    "seriesVals = series.values\n",
    "trainPercent = 0.8 # 80% of the data is used for training\n",
    "\n",
    "#Split into train and test\n",
    "trainBegin = int(trainPercent*len(seriesVals)) \n",
    "train = seriesVals[0:trainBegin]\n",
    "test = seriesVals[trainBegin:]\n",
    "np.random.shuffle(train) # shuffle the training dataset\n",
    "\n",
    "# Split into x and y\n",
    "xTrain,yTrain = train[:,0:-1],train[:,-1] # X is the first lag elements. Y is the lag+1 element\n",
    "xTest,yTest = test[:,0:-1],test[:,-1] # Same for testing data\n",
    "\n",
    "\n",
    "#Reshape for keras\n",
    "xTrain = xTrain.reshape((xTrain.shape[0], xTrain.shape[1], 1))\n",
    "xTest = xTest.reshape(xTest.shape[0], xTest.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Function\n",
    "\n",
    "We need to define a custom loss function for our classifier. It needs to be based on backtesting to maximize profit and maximize sharpe ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom Loss function that uses Backtrader"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AlgoTrading]",
   "language": "python",
   "name": "conda-env-AlgoTrading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
